{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demonstrated-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import enchant\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import re \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acting-lottery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-enterprise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neither-capability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEaCAYAAAAR0SDgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT30lEQVR4nO3df7Ddd13n8eeLBEoBi629rTUpJECW0lahNJagO4pUt3FRUpVqukKjUycztQqoo6aOIzNi1uKP7tLRZskKNlWkm6loo05Zu7HoIIXubYuENtRGgm1obC4gJVappH3vH+eTndObk+Tee8L5ntvzfMycOd/v+/v9nrwzt+nrfj/fz/l+U1VIkvSsrhuQJI0HA0GSBBgIkqTGQJAkAQaCJKkxECRJACztuoGFOv3002vFihVdtyFJi8rdd9/9+aqaGrRt0QbCihUrmJ6e7roNSVpUkvzj0bY5ZCRJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSc2i/WLaqK3Y9Bddt/A19dlr39B1C5I65hmCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDXHDYQk70tyIMmn+mqnJbk9yYPt/dS+bdck2ZPkgSSX9NUvTLKrbbs+SVr9pCT/q9U/nmTFCf47SpLmYC5nCDcCa2fVNgE7q2oVsLOtk+RcYD1wXjvmhiRL2jFbgI3AqvY6/JlXAv9cVS8D/hvwroX+ZSRJC3fcQKiqvwG+OKu8DtjWlrcBl/bVb66qJ6pqL7AHuCjJWcApVXVnVRVw06xjDn/WLcDFh88eJEmjs9BrCGdW1X6A9n5Gqy8DHu7bb1+rLWvLs+tPO6aqDgGPAd+wwL4kSQt0oi8qD/rNvo5RP9YxR354sjHJdJLpmZmZBbYoSRpkoYHwaBsGor0faPV9wNl9+y0HHmn15QPqTzsmyVLghRw5RAVAVW2tqtVVtXpqamqBrUuSBlloIOwANrTlDcCtffX1bebQSnoXj+9qw0oHk6xp1weumHXM4c96E/BX7TqDJGmElh5vhyQfAF4HnJ5kH/AO4Fpge5IrgYeAywCq6r4k24H7gUPA1VX1ZPuoq+jNWDoZuK29AN4L/EGSPfTODNafkL+ZJGlejhsIVXX5UTZdfJT9NwObB9SngfMH1L9CCxRJUnf8prIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgBY2nUD0iis2PQXXbfwNfPZa9/QdQt6hvAMQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRIwZCAk+Zkk9yX5VJIPJHluktOS3J7kwfZ+at/+1yTZk+SBJJf01S9Msqttuz5JhulLkjR/Cw6EJMuAtwKrq+p8YAmwHtgE7KyqVcDOtk6Sc9v284C1wA1JlrSP2wJsBFa119qF9iVJWphhh4yWAicnWQo8D3gEWAdsa9u3AZe25XXAzVX1RFXtBfYAFyU5Czilqu6sqgJu6jtGkjQiCw6Eqvoc8FvAQ8B+4LGq+kvgzKra3/bZD5zRDlkGPNz3EftabVlbnl0/QpKNSaaTTM/MzCy0dUnSAMMMGZ1K77f+lcA3Ac9P8uZjHTKgVseoH1ms2lpVq6tq9dTU1HxbliQdwzBDRt8N7K2qmar6KvBB4NuAR9swEO39QNt/H3B23/HL6Q0x7WvLs+uSpBEaJhAeAtYkeV6bFXQxsBvYAWxo+2wAbm3LO4D1SU5KspLexeO72rDSwSRr2udc0XeMJGlEFnz766r6eJJbgHuAQ8C9wFbgBcD2JFfSC43L2v73JdkO3N/2v7qqnmwfdxVwI3AycFt7SdIz+tblMF63Lx/qeQhV9Q7gHbPKT9A7Wxi0/2Zg84D6NHD+ML1IkobjN5UlSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKaoQIhydcnuSXJp5PsTvLaJKcluT3Jg+391L79r0myJ8kDSS7pq1+YZFfbdn2SDNOXJGn+hj1DeDfwoao6B3glsBvYBOysqlXAzrZOknOB9cB5wFrghiRL2udsATYCq9pr7ZB9SZLmacGBkOQU4DuA9wJU1b9X1ZeAdcC2tts24NK2vA64uaqeqKq9wB7goiRnAadU1Z1VVcBNfcdIkkZkmDOElwAzwO8nuTfJ7yV5PnBmVe0HaO9ntP2XAQ/3Hb+v1Za15dn1IyTZmGQ6yfTMzMwQrUuSZhsmEJYCrwa2VNUFwOO04aGjGHRdoI5RP7JYtbWqVlfV6qmpqfn2K0k6hmECYR+wr6o+3tZvoRcQj7ZhINr7gb79z+47fjnwSKsvH1CXJI3QggOhqv4JeDjJy1vpYuB+YAewodU2ALe25R3A+iQnJVlJ7+LxXW1Y6WCSNW120RV9x0iSRmTpkMf/NPD+JM8BPgP8OL2Q2Z7kSuAh4DKAqrovyXZ6oXEIuLqqnmyfcxVwI3AycFt7SZJGaKhAqKpPAKsHbLr4KPtvBjYPqE8D5w/TiyRpOH5TWZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqRk6EJIsSXJvkj9v66cluT3Jg+391L59r0myJ8kDSS7pq1+YZFfbdn2SDNuXJGl+TsQZwtuA3X3rm4CdVbUK2NnWSXIusB44D1gL3JBkSTtmC7ARWNVea09AX5KkeRgqEJIsB94A/F5feR2wrS1vAy7tq99cVU9U1V5gD3BRkrOAU6rqzqoq4Ka+YyRJIzLsGcJ/B34BeKqvdmZV7Qdo72e0+jLg4b799rXasrY8uy5JGqEFB0KS7wMOVNXdcz1kQK2OUR/0Z25MMp1kemZmZo5/rCRpLoY5Q/h24I1JPgvcDLw+yR8Cj7ZhINr7gbb/PuDsvuOXA4+0+vIB9SNU1daqWl1Vq6empoZoXZI024IDoaquqarlVbWC3sXiv6qqNwM7gA1ttw3ArW15B7A+yUlJVtK7eHxXG1Y6mGRNm110Rd8xkqQRWfo1+Mxrge1JrgQeAi4DqKr7kmwH7gcOAVdX1ZPtmKuAG4GTgdvaS5I0QickEKrqw8CH2/IXgIuPst9mYPOA+jRw/onoRZK0MH5TWZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqVlwICQ5O8kdSXYnuS/J21r9tCS3J3mwvZ/ad8w1SfYkeSDJJX31C5PsatuuT5Lh/lqSpPka5gzhEPBzVfUKYA1wdZJzgU3AzqpaBexs67Rt64HzgLXADUmWtM/aAmwEVrXX2iH6kiQtwIIDoar2V9U9bfkgsBtYBqwDtrXdtgGXtuV1wM1V9URV7QX2ABclOQs4parurKoCbuo7RpI0IifkGkKSFcAFwMeBM6tqP/RCAzij7bYMeLjvsH2ttqwtz65LkkZo6EBI8gLgj4G3V9WXj7XrgFodoz7oz9qYZDrJ9MzMzPyblSQd1VCBkOTZ9MLg/VX1wVZ+tA0D0d4PtPo+4Oy+w5cDj7T68gH1I1TV1qpaXVWrp6amhmldkjTLMLOMArwX2F1V1/Vt2gFsaMsbgFv76uuTnJRkJb2Lx3e1YaWDSda0z7yi7xhJ0ogsHeLYbwfeAuxK8olW+yXgWmB7kiuBh4DLAKrqviTbgfvpzVC6uqqebMddBdwInAzc1l6SpBFacCBU1UcYPP4PcPFRjtkMbB5QnwbOX2gvkqTh+U1lSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAFjFAhJ1iZ5IMmeJJu67keSJs1YBEKSJcDvAt8LnAtcnuTcbruSpMkyFoEAXATsqarPVNW/AzcD6zruSZImytKuG2iWAQ/3re8DXjN7pyQbgY1t9V+SPDCC3rpyOvD5Uf1hedeo/qSJ4M9ucXum//xefLQN4xIIGVCrIwpVW4GtX/t2updkuqpWd92H5s+f3eI2yT+/cRky2gec3be+HHiko14kaSKNSyD8X2BVkpVJngOsB3Z03JMkTZSxGDKqqkNJfgr438AS4H1VdV/HbXVtIobGnqH82S1uE/vzS9URQ/WSpAk0LkNGkqSOGQiSJMBAkCQ1BoIkAUlOTvLyrvvokoEgnQDpeXOSX2nrL0pyUdd9aW6SfD/wCeBDbf1VSSZu6ruzjMZAkoMM+GY2vW9wV1WdMuKWNE9JtgBPAa+vqlckORX4y6r61o5b0xwkuRt4PfDhqrqg1T5ZVd/SbWejNRbfQ5h0VfV1Xfegob2mql6d5F6Aqvrn9iVLLQ6HquqxZNBddCaHgTCGkpwBPPfwelU91GE7mpuvttu4F0CSKXpnDFocPpXkvwBLkqwC3gp8tOOeRs5rCGMkyRuTPAjsBf4a+CxwW6dNaa6uB/4EOCPJZuAjwH/ttiXNw08D5wFPAH8EPAa8vcuGuuA1hDGS5O/ojWP+n6q6IMl3AZdX1cbjHKoxkOQc4GJ61352VtXujlvSHCW5oKru7bqPrnmGMF6+WlVfAJ6V5FlVdQfwqo570hwkeTdwWlX9blX9jmGw6FyX5NNJ3pnkvK6b6YqBMF6+lOQFwN8A72//kznUcU+am3uAX27PBP/NJBN5P/3Fqqq+C3gdMANsTbIryS9329XoOWQ0RpI8H/g3ekH9o8ALgfe3swYtAklOA36I3i3cX1RVqzpuSfOU5JuBXwB+pKomaqaYs4zGRJuhcmtVfTe92SnbOm5JC/My4BxgBXB/t61orpK8AvgR4E3AF+g91/3nOm2qAwbCmKiqJ5P8a5IXVtVjXfej+UnyLuAHgX8AtgPvrKovddqU5uP3gQ8A/6mqJvZpjQbCePkKsCvJ7cDjh4tV9dbuWtIc7QVeW1Ujezi7TpyqWtN1D+PAawhjJMmGAeWqqptG3ozmJMk5VfXpJK8etL2q7hl1T5q7JNur6oeT7OLpt485fNsYb12hznx9Vb27v5DkbV01ozn5WWAj8NsDthW975VofB3+9/V9nXYxJjxDGCNJ7qmqV8+q3Xv4ZlsaX0meW1VfOV5N4ynJu6rqF49Xe6bzewhjIMnlSf4MWJlkR9/rDnozHjT+Bt33ZuLuhbOIfc+A2veOvIuOOWQ0Hj4K7AdO5+lDDweBT3bSkeYkyTcCy4CTk1xAb+wZ4BTgeZ01pjlJchXwk8BLkvT/W/s64G+76ao7DhlJQ2gTAX4MWA1M9206CNxYVR/soi/NTZIXAqcCvw5s6tt0sKq+2E1X3TEQxsisB+U8B3g28LgPyBl/SX6oqv646z40nEm/9bxDRmNk9oNyklwK+BjGMZbkzVX1h8CKJD87e3tVXddBW5qn9gjN64BvAg4ALwZ207sl9sTwovIYq6o/xWmL4+757f0F9MadZ7+0OPwasAb4+6paSe825l5DUHeS/GDf6rPojUt/Z1W9tqOWpImQZLqqVrdnklxQVU8luauqJuoM3SGj8fL9fcuH6D0xbV03rWg+kvwGvd8y/w34EPBK4O1tOEnjb/at5w8wgbee9wxBOgGSfKKqXpXkB4BLgZ8B7qiqV3bbmeai3Xr+K/SmDU/srec9QxgjSf4DsAU4s6rOT/ItwBur6tc6bk3H9+z2/p+BD1TVF5Mca3+Nkap6vG91Ym8970Xl8fI/gWuArwJU1SfpPWhF4+/Pknya3nWfnUmm6P3GqUUgycEkX571ejjJnyR5Sdf9jYpnCOPleVV116zfLCduHHMxqqpN7ZkIX27Ptngcr/8sJtcBjwB/RG/YaD3wjcADwPvoPV7zGc9AGC+fT/JS2pfTkryJ3i0tNOaSPBt4C/AdLdD/GvgfnTal+VhbVa/pW9+a5GNV9atJfqmzrkbMQBgvVwNbgXOSfI7eQ1d+tNuWNEdb6F1HuKGtv6XVfqKzjjQfTyX5YeCWtv6mvm0TM/PGWUZjJMlJ9P5DXAGcBnyZ3kM6frXLvnR8Sf5u9oyiQTWNp3ad4N3Aa+kFwMfozRT7HHBhVX2kw/ZGxjOE8XIr8CXgHnrjmVo8nkzy0qr6B/j//4N5suOeNEdV9Rme/j2gfhMRBmAgjJvlVbW26ya0ID8P3JHkM219BfDj3bWj+XDKd4/TTsfLR5N8c9dNaEH+FngP8FR7vQe4s9OONB9O+cYzhHHzH4EfS7IXeIIJfdD3InUTvWs+72zrlwN/AFzWWUeaD6d8YyCMm4l7ZN8zyMtnXUC+o90oTYuDU74xEMZKVf1j1z1owe5NsqaqPgaQ5DVM4O2TFzGnfOO0U+mESLIbeDlw+AlbL6L3gJWncNhv7Dnlu8czBOnEcHbY4uaUbzxDkCSSfKqqzu+6j6457VSSnPINeIYgSSS5H3gZvYvJEzvl20CQNPGSvHhQfdJm/hkIkiTAawiSpMZAkCQBBoIkqTEQJEmAgSBJav4fkkWXVwyZmCMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train.sentiment.value_counts().plot(kind = 'bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-respect",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "virtual-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(df):\n",
    "\n",
    "    df.drop('textID', axis= 1 , inplace = True)\n",
    "    df.dropna(axis=0, inplace= True)\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else(1 if x == 'neutral' else 2))\n",
    "    df['text_clean'] = df['text'].str.lower()\n",
    "    df['text_clean'] = df['text_clean'].apply(lambda x: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(x)))  \n",
    "    df['text_clean'] = df['text_clean'].apply(lambda x: re.sub(r\"\\d+\", \"\", x))\n",
    "    df['text_tokonize'] = df['text_clean'].apply(lambda x: word_tokenize(x))\n",
    "    return df\n",
    "df_train = text_cleaning(df_train)\n",
    "df_test = text_cleaning(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inclusive-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text=[word for word in text if word not in stop]\n",
    "    return text\n",
    "\n",
    "df_train['text_withNoStopWords'] = df_train['text_tokonize'].apply(lambda x: remove_stopwords(x))\n",
    "df_test['text_withNoStopWords'] = df_test['text_tokonize'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rough-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN) \n",
    "\n",
    "def lemmitizer(x):\n",
    "    x = [WordNetLemmatizer().lemmatize(word, get_pos(word)) for word in x ]\n",
    "    return x\n",
    "\n",
    "df_train['clean_text'] = df_train['text_withNoStopWords'].apply(lambda x:lemmitizer(x))\n",
    "df_test['clean_text'] = df_test['text_withNoStopWords'].apply(lambda x:lemmitizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nearby-interval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokonize</th>\n",
       "      <th>text_withNoStopWords</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[id, have, responded, if, i, were, going]</td>\n",
       "      <td>[id, responded, going]</td>\n",
       "      <td>[id, respond, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>0</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>0</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me]</td>\n",
       "      <td>[boss, bullying]</td>\n",
       "      <td>[bos, bullying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[what, interview, leave, me, alone]</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>0</td>\n",
       "      <td>sons of  why couldnt they put them on the rel...</td>\n",
       "      <td>[sons, of, why, couldnt, they, put, them, on, ...</td>\n",
       "      <td>[sons, couldnt, put, releases, already, bought]</td>\n",
       "      <td>[son, couldnt, put, release, already, bought]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                I`d have responded, if I were going   \n",
       "1      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                          my boss is bullying me...   \n",
       "3                     what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text  sentiment  \\\n",
       "0  I`d have responded, if I were going          1   \n",
       "1                             Sooo SAD          0   \n",
       "2                          bullying me          0   \n",
       "3                       leave me alone          0   \n",
       "4                        Sons of ****,          0   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                  id have responded if i were going   \n",
       "1         sooo sad i will miss you here in san diego   \n",
       "2                             my boss is bullying me   \n",
       "3                      what interview leave me alone   \n",
       "4   sons of  why couldnt they put them on the rel...   \n",
       "\n",
       "                                       text_tokonize  \\\n",
       "0          [id, have, responded, if, i, were, going]   \n",
       "1  [sooo, sad, i, will, miss, you, here, in, san,...   \n",
       "2                       [my, boss, is, bullying, me]   \n",
       "3                [what, interview, leave, me, alone]   \n",
       "4  [sons, of, why, couldnt, they, put, them, on, ...   \n",
       "\n",
       "                              text_withNoStopWords  \\\n",
       "0                           [id, responded, going]   \n",
       "1                    [sooo, sad, miss, san, diego]   \n",
       "2                                 [boss, bullying]   \n",
       "3                        [interview, leave, alone]   \n",
       "4  [sons, couldnt, put, releases, already, bought]   \n",
       "\n",
       "                                      clean_text  \n",
       "0                              [id, respond, go]  \n",
       "1                  [sooo, sad, miss, san, diego]  \n",
       "2                                [bos, bullying]  \n",
       "3                      [interview, leave, alone]  \n",
       "4  [son, couldnt, put, release, already, bought]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greatest-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = enchant.Dict(\"en_US\")\n",
    "def checkIfEnglish(text):\n",
    "    text=[word for word in text if d.check(word)==True]    \n",
    "    # initialize an empty string\n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string  \n",
    "    return (str1.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "derived-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean_text'] = df_train['clean_text'].apply(lambda x:  checkIfEnglish(x))\n",
    "df_test['clean_text'] = df_test['clean_text'].apply(lambda x: checkIfEnglish(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-voice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokonize</th>\n",
       "      <th>text_withNoStopWords</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[id, have, responded, if, i, were, going]</td>\n",
       "      <td>[id, responded, going]</td>\n",
       "      <td>id respond go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>0</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "      <td>sad miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>0</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, boss, is, bullying, me]</td>\n",
       "      <td>[boss, bullying]</td>\n",
       "      <td>bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[what, interview, leave, me, alone]</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "      <td>interview leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>0</td>\n",
       "      <td>sons of  why couldnt they put them on the rel...</td>\n",
       "      <td>[sons, of, why, couldnt, they, put, them, on, ...</td>\n",
       "      <td>[sons, couldnt, put, releases, already, bought]</td>\n",
       "      <td>son put release already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                I`d have responded, if I were going   \n",
       "1      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                          my boss is bullying me...   \n",
       "3                     what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text  sentiment  \\\n",
       "0  I`d have responded, if I were going          1   \n",
       "1                             Sooo SAD          0   \n",
       "2                          bullying me          0   \n",
       "3                       leave me alone          0   \n",
       "4                        Sons of ****,          0   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                  id have responded if i were going   \n",
       "1         sooo sad i will miss you here in san diego   \n",
       "2                             my boss is bullying me   \n",
       "3                      what interview leave me alone   \n",
       "4   sons of  why couldnt they put them on the rel...   \n",
       "\n",
       "                                       text_tokonize  \\\n",
       "0          [id, have, responded, if, i, were, going]   \n",
       "1  [sooo, sad, i, will, miss, you, here, in, san,...   \n",
       "2                       [my, boss, is, bullying, me]   \n",
       "3                [what, interview, leave, me, alone]   \n",
       "4  [sons, of, why, couldnt, they, put, them, on, ...   \n",
       "\n",
       "                              text_withNoStopWords  \\\n",
       "0                           [id, responded, going]   \n",
       "1                    [sooo, sad, miss, san, diego]   \n",
       "2                                 [boss, bullying]   \n",
       "3                        [interview, leave, alone]   \n",
       "4  [sons, couldnt, put, releases, already, bought]   \n",
       "\n",
       "                       clean_text  \n",
       "0                   id respond go  \n",
       "1                        sad miss  \n",
       "2                        bullying  \n",
       "3           interview leave alone  \n",
       "4  son put release already bought  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dirty-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean_text'] = df_train['clean_text'].apply(lambda x: x if len(x) >=2 else None )\n",
    "df_test['clean_text'] = df_test['clean_text'].apply(lambda x: x if len(x) >= 2 else None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "purple-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(axis=0, inplace=True)\n",
    "df_test.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "flush-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X_Y(df):\n",
    "    X = df['clean_text']\n",
    "    Y = df['sentiment']\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = split_X_Y(df_train)\n",
    "X_test, Y_test = split_X_Y(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "smaller-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,1))\n",
    "count_vect_train = count_vect.fit_transform(X_train)\n",
    "count_vect_train = count_vect_train.toarray()\n",
    "count_vect_test = count_vect.transform(X_test)\n",
    "count_vect_test = count_vect_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "extreme-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-monitoring",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "regular-filename",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
       "                   n_jobs=-1, random_state=42, solver='newton-cg')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg', n_jobs = -1,\n",
    "                                     class_weight = 'balanced', C = 0.1, random_state = 42)\n",
    "model.fit(count_vect_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-retrieval",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "referenced-blues",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.7002944543924225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       996\n",
      "           1       0.65      0.69      0.67      1384\n",
      "           2       0.79      0.72      0.75      1097\n",
      "\n",
      "    accuracy                           0.70      3477\n",
      "   macro avg       0.71      0.70      0.70      3477\n",
      "weighted avg       0.70      0.70      0.70      3477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(count_vect_test)\n",
    "\n",
    "score = f1_score(Y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(\"f1 score: {}\".format(score))\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
